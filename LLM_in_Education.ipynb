{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3yxHCOP9A3Tc6CqtcNuG3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adharshrj/llmsinedu/blob/main/LLM_in_Education.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation"
      ],
      "metadata": {
        "id": "kCAcf27qZ_1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pip installation LangChain and Hugginface API\n",
        "!pip install langchain\n",
        "!pip install huggingface_hub\n",
        "\n",
        "# Pip installation of additional needed libraries\n",
        "!pip install sentence_transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install \"unstructured[all-docs]\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuBNV8gXaBnj",
        "outputId": "df09de32-3c87-4093-9c60-c077ef337c6d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.14 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.16)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.16)\n",
            "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.83)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.14)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.11.17)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (10.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)\n",
            "Requirement already satisfied: unstructured[all-docs] in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.11.2)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.10.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.6.3)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2024.1.2)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.23.5)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.6.1)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.9.0)\n",
            "Requirement already satisfied: unstructured-client>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.14.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.5.3)\n",
            "Requirement already satisfied: msg-parser in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.2.0)\n",
            "Requirement already satisfied: pikepdf in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (8.11.2)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.0.0)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.15.0)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.5.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.2.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.1.2)\n",
            "Requirement already satisfied: unstructured-inference==0.7.23 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.7.23)\n",
            "Requirement already satisfied: pypandoc in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.12)\n",
            "Requirement already satisfied: unstructured.pytesseract>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.3.12)\n",
            "Requirement already satisfied: python-pptx<=0.6.23 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.6.23)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.17.0)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.0.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.1.0)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (20221105)\n",
            "Requirement already satisfied: layoutparser[layoutmodels,tesseract] in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.23->unstructured[all-docs]) (0.3.4)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.23->unstructured[all-docs]) (0.0.6)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.23->unstructured[all-docs]) (0.20.3)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.23->unstructured[all-docs]) (4.8.0.76)\n",
            "Requirement already satisfied: onnxruntime<1.16 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.23->unstructured[all-docs]) (1.15.1)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.23->unstructured[all-docs]) (4.35.2)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx<=0.6.23->unstructured[all-docs]) (10.2.0)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from python-pptx<=0.6.23->unstructured[all-docs]) (3.1.9)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[all-docs]) (2023.11.17)\n",
            "Requirement already satisfied: charset-normalizer>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[all-docs]) (3.3.2)\n",
            "Requirement already satisfied: idna>=3.4 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[all-docs]) (3.6)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[all-docs]) (1.0.6)\n",
            "Requirement already satisfied: marshmallow>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[all-docs]) (3.20.2)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[all-docs]) (1.0.0)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[all-docs]) (23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[all-docs]) (2.8.2)\n",
            "Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[all-docs]) (1.16.0)\n",
            "Requirement already satisfied: typing-inspect>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[all-docs]) (0.9.0)\n",
            "Requirement already satisfied: urllib3>=1.26.18 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured[all-docs]) (2.0.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[all-docs]) (2.5)\n",
            "Requirement already satisfied: olefile>=0.46 in /usr/local/lib/python3.10/dist-packages (from msg-parser->unstructured[all-docs]) (0.47)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (4.66.1)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx->unstructured[all-docs]) (3.20.3)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->unstructured[all-docs]) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2023.3.post1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[all-docs]) (42.0.0)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from pikepdf->unstructured[all-docs]) (1.2.14)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (1.16.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.23->unstructured[all-docs]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.23->unstructured[all-docs]) (23.5.26)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<1.16->unstructured-inference==0.7.23->unstructured[all-docs]) (1.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.23->unstructured[all-docs]) (3.13.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.23->unstructured[all-docs]) (6.0.1)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.23->unstructured[all-docs]) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.23->unstructured[all-docs]) (0.4.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unstructured-inference==0.7.23->unstructured[all-docs]) (2023.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[all-docs]) (1.11.4)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[all-docs]) (0.1.10)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[all-docs]) (0.10.3)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[all-docs]) (0.3.10)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[all-docs]) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[all-docs]) (0.16.0+cu121)\n",
            "Requirement already satisfied: effdet in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[all-docs]) (0.4.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (2.21)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime<1.16->unstructured-inference==0.7.23->unstructured[all-docs]) (10.0)\n",
            "Requirement already satisfied: timm>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[all-docs]) (0.9.12)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[all-docs]) (2.0.7)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[all-docs]) (2.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[all-docs]) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[all-docs]) (2.1.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[all-docs]) (2.8.2)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[all-docs]) (4.26.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<1.16->unstructured-inference==0.7.23->unstructured[all-docs]) (1.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[all-docs]) (4.9.3)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[all-docs]) (3.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[all-docs]) (2.1.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[all-docs]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[all-docs]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[all-docs]) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[all-docs]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.23->unstructured[all-docs]) (3.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Env Setup"
      ],
      "metadata": {
        "id": "lDUats7_aH8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\"\n",
        "os.environ[\"HF_TOKEN\"] = \"\""
      ],
      "metadata": {
        "id": "lb9Jy9LUaLDu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect Google Drive"
      ],
      "metadata": {
        "id": "WCWZGR2waNhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brSCSAe-aOwe",
        "outputId": "652d2142-0fcd-4e62-da3e-4f4d7288560c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup Loaders"
      ],
      "metadata": {
        "id": "YK-LIyqJaYiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.document_loaders import OnlinePDFLoader"
      ],
      "metadata": {
        "id": "Jh5gTt-Qahni"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadPDFFromLocal(pdf_file_path=\"/content/drive/MyDrive/LLM_Testing_Docs/clrs.pdf\"):\n",
        "    loader = PyPDFLoader(pdf_file_path)\n",
        "    pages = loader.load_and_split()\n",
        "\n",
        "    # Adding progress tracking\n",
        "    total_pages = len(pages)\n",
        "    for i, page in enumerate(pages):\n",
        "        print(f\"Processing page {i+1} of {total_pages}\")\n",
        "\n",
        "    return pages\n"
      ],
      "metadata": {
        "id": "VIlO9H9bajY_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadFromUrl(url=\"https://www.nrel.gov/docs/fy12osti/55871.pdf\"):\n",
        "  onlineLoader = OnlinePDFLoader(url)\n",
        "  newPg = onlineLoader.load_and_split()\n",
        "\n",
        "  print(newPg)\n",
        "  return newPg"
      ],
      "metadata": {
        "id": "16fSqFB9f9z-"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split Documents (LLMS cannot read large amounts of data)"
      ],
      "metadata": {
        "id": "kXa2u-ngap3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter"
      ],
      "metadata": {
        "id": "wBWWZdtMaw17"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitDocument(loaded_docs):\n",
        "    # Splitting documents into chunks\n",
        "    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
        "    chunked_docs = splitter.split_documents(loaded_docs)\n",
        "    return chunked_docs"
      ],
      "metadata": {
        "id": "CoOfOT0OayrI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Embeddings"
      ],
      "metadata": {
        "id": "m_6W4pBOcBRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "OglJFVL9cDTh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createEmbeddings(chunked_docs):\n",
        "    # Create embeddings and store them in a FAISS vector store\n",
        "    embedder = HuggingFaceEmbeddings()\n",
        "    vector_store = FAISS.from_documents(chunked_docs, embedder)\n",
        "    return vector_store"
      ],
      "metadata": {
        "id": "JXMovxhTcEDK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use those embeddings to feed the LLM model and Answer Questions"
      ],
      "metadata": {
        "id": "gG6T35zWa3vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain import HuggingFaceHub"
      ],
      "metadata": {
        "id": "7fldXptla8fQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadLLMModel():\n",
        "    llm=HuggingFaceHub(repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", model_kwargs={\"temperature\":0, \"max_length\":2048})\n",
        "    chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "    return chain\n",
        "\n",
        "def askQuestions(vector_store, chain, question):\n",
        "    # Ask a question using the QA chain\n",
        "    similar_docs = vector_store.similarity_search(question)\n",
        "    response = chain.run(input_documents=similar_docs, question=question)\n",
        "    return response"
      ],
      "metadata": {
        "id": "wWFvKJT9a_vn"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = loadLLMModel()"
      ],
      "metadata": {
        "id": "J1PZyCqkbDPi"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "kndUK6XqbF9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PDF_loaded_docs = loadPDFFromLocal()\n",
        "PDF_chunked_docs = splitDocument(PDF_loaded_docs)\n",
        "PDF_vector_store = createEmbeddings(PDF_loaded_docs)"
      ],
      "metadata": {
        "id": "PGdH7wKkkLeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PDF_loaded_docs = loadFromUrl()\n",
        "PDF_vector_store = createEmbeddings(PDF_loaded_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl0DNvVpjZlR",
        "outputId": "6d96b415-3f21-41d7-f3a5-3bca30e943c8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='Wind Power Plant Prediction by Using Neural Networks\\n\\nPreprint\\n\\nZ. Liu and W. Gao University of Denver\\n\\nY.-H. Wan and E. Muljadi National Renewable Energy Laboratory\\n\\nTo be presented at the IEEE Energy Conversion Conference and Exposition Raleigh, North Carolina September 15–20, 2012\\n\\nNREL is a national laboratory of the U.S. Department of Energy, Office of Energy Efficiency & Renewable Energy, operated by the Alliance for Sustainable Energy, LLC.\\n\\nConference Paper NREL/CP-5500-55871 August 2012\\n\\nContract No. DE-AC36-08GO28308\\n\\nNOTICE\\n\\nThe submitted manuscript has been offered by an employee of the Alliance for Sustainable Energy, LLC (Alliance), a contractor of the US Government under Contract No. DE-AC36-08GO28308. Accordingly, the US Government and Alliance retain a nonexclusive royalty-free license to publish or reproduce the published form of this contribution, or allow others to do so, for US Government purposes.\\n\\nThis report was prepared as an account of work sponsored by an agency of the United States government. Neither the United States government nor any agency thereof, nor any of their employees, makes any warranty, express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States government or any agency thereof. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States government or any agency thereof.\\n\\nAvailable electronically at http://www.osti.gov/bridge\\n\\nAvailable for a processing fee to U.S. Department of Energy and its contractors, in paper, from:\\n\\nU.S. Department of Energy Office of Scientific and Technical Information P.O. Box 62 Oak Ridge, TN 37831-0062 phone: 865.576.8401 fax: 865.576.5728 email: mailto:reports@adonis.osti.gov\\n\\nAvailable for sale to the public, in paper, from:\\n\\nU.S. Department of Commerce National Technical Information Service 5285 Port Royal Road Springfield, VA 22161 phone: 800.553.6847 fax: 703.605.6900 email: orders@ntis.fedworld.gov online ordering: http://www.ntis.gov/help/ordermethods.aspx\\n\\nCover Photos: (left to right) PIX 16416, PIX 17423, PIX 16560, PIX 17613, PIX 17436, PIX 17721\\n\\nPrinted on paper containing at least 50% wastepaper, including 10% post consumer waste.\\n\\nWind Power Plant Prediction by Using Neural Networks\\n\\nZiqiao Liu, Student Member, IEEE, Wenzhong Gao, Senior Member, IEEE, Yih-Huei Wan, Senior Member, IEEE, Eduard Muljadi, Fellow, IEEE\\n\\nAbstract--This paper introduces a method of short term wind power prediction for a wind power plant by training neural networks based on historical data of wind speed and wind direction. There are two steps in the process of wind power prediction. In the first step, raw data collected by plant information system is filtered by probabilistic neural network. This step prepares valid data to be used for building a prediction model. In the second step, a complex-valued recurrent neural network is applied to build a model to predict wind power. The test results of the prediction model are presented and analyzed at the end of the paper. The model proposed is shown to achieve a high accuracy with respect to the measured data.\\n\\nIndex Terms--wind power plant, wind power prediction, probabilistic neural network, complex-valued recurrent neural network.\\n\\nI. INTRODUCTION\\n\\nW', metadata={'source': '/tmp/tmp06m3zwow/tmp.pdf'}), Document(page_content=\"Index Terms--wind power plant, wind power prediction, probabilistic neural network, complex-valued recurrent neural network.\\n\\nI. INTRODUCTION\\n\\nW\\n\\nIND plant has lower cost of energy compared to other renewable energy sources for large scale application. Due to the different geographical patterns, weather, and properties of the wind turbines, a wind turbine may have various performance given different situations. If the total output of a wind power plant (WPP) can be predicted with high accuracy, more useful information can be provided to the power companies to help in scheduling power generation. This information will allow a more flexible and intelligent control of a WPP (e.g., improve the working schedule of wind turbines, reactive power control, etc). Methods for predicting wind power generation can be categorized into physical methods, statistical methods, methods based on neural networks, and hybrid methods [1]. The physical methods rely heavily on numeric whether prediction, which is confined by the sensors and monitoring devices placed within the WPP. The quality of hardware chosen, the parameter settings, the computation time, the time delays, and the sampling rates influence the accuracy of data collected from the WPP. It is easier to predict a single wind turbine's performance rather than a whole WPP's power generation. Statistical and neural network methods are based on the historical data and have a low prediction cost. The relationship between input data and\\n\\noutput data based on historical measured data is learned and then a nonlinear relationship model between them is built. But when new data not previously included in the training data set is used as input into this kind of model, the prediction error might be large, which is a disadvantage. Different prediction methods mentioned above can be combined as hybrid methods to achieve better prediction results. But this will increase the complexity of the model. In this paper, several neural network methods are applied to predict power generation of a WPP located in northeast Colorado.\\n\\nIn this WPP, data of wind information, such as wind speed, wind direction, wind power generation, humidity and air pressure are collected by a Plant Information (PI) system, and the output of the entire WPP is monitored by the utility's supervisory control and data acquisition (SCADA) system. Raw data from the WPP is processed by a probabilistic neural network (PNN) and then a complex-valued recurrent neural network (CRNN) model is built to predict the total output of the WPP with the following considerations [3]: • The raw data set will be screened by probabilistic neural network to prepare high quality data for building neural network models;\\n\\nThe model's inputs do not rely on the data of wind speed and wind direction from all turbines; representative wind turbines can be found to compress the length of the input data;\\n\\nThe inputs are expressed as complex-valued data (vector representation) which combine wind speed and wind direction;\\n\\nThe complex-valued recurrent neural network model's time series inputs are generated based on the historical data values of the WPP rather than the predicted values by the model at the previous steps;\\n\\nThe result to be predicted is the total power generation of the whole WPP rather than outputs of some single wind turbines;\\n\\nThe models are trained based on the data collected within the year of 2010; the prediction accuracy of the model is tested by the data of 2011.\\n\\nThis work was supported in part by U.S. DOE Contract No. DE-AC36-08-\\n\\nGO28308 and in part by U.S. NSF Grant 0844707.\\n\\nZ. Liu and W. Gao are with Department of Electrical and Computer\\n\\nEngineering, University of Denver, Denver, CO, 80210 USA\", metadata={'source': '/tmp/tmp06m3zwow/tmp.pdf'}), Document(page_content='GO28308 and in part by U.S. NSF Grant 0844707.\\n\\nZ. Liu and W. Gao are with Department of Electrical and Computer\\n\\nEngineering, University of Denver, Denver, CO, 80210 USA\\n\\nThe rest of the paper are arranged as follows. In section II, the data preparation process of wind power plant by using PNN is introduced. In section III, we show the process of wind power prediction model design by using complex-valued recurrent neural network. In section IV, prediction results using CRNN are discussed and analyzed. The conclusion of the paper is summarized in section V.\\n\\n(E-mail: Ziqiao.Liu@du.edu and Wenzhong.Gao@du.edu). Y. Wan and E. Muljadi are with National Renewable Energy Laboratory, and\\n\\nGolden, CO 80401, USA Eduard.Muljadi@nrel.gov).\\n\\n(e-mail: Yih-Huei.Wan@nrel.gov\\n\\n1\\n\\nII. DATA PREPARATION\\n\\nA data preparation process is a very important step in mathematical modeling, since the quality of raw data acquired by PI system may contain errors.\\n\\nA. Raw Data Description and Analysis\\n\\nThere are two kinds of wind turbines in this WPP. There are 53 turbines in Group 1 with each turbine’s rated power at 1.5 MW; there are 221 turbines in Group 2 with each rated at 1MW. The rated power of the whole WPP is 300.5 MW. The layout of wind turbines and two meteorological towers (MET1&2) is shown in Fig. 1.\\n\\nFig.1. Wind power plant distribution\\n\\nThe data of wind speed (m/s), wind direction (degree, 0o~360o), total metered plant-output power (MW), temperature (oC) and air pressure is monitored by the sensors installed at the two MET towers. From individual wind turbines, data of wind speed, wind direction and power output is also collected. The data of total metered WPP output power is recorded at the point of interconnection and is very useful to a utility company as a reference to compute power revenue. Following the IEC standard, all the data acquired except the turbine status is averaged over a 10-minute period for turbine power curve measurement [1]. Fig. 2 shows a raw scatter plot of WPP output and wind speed data from MET 1. The raw data set (8486 dots) contains some invalid data which is not useful for power prediction and has a minor effect on the power grid. The raw data can be classified into five types as shown in Table I.\\n\\nTABLE I RAW DATA CLASSIFICATION\\n\\nType 1 2 3 4 5\\n\\nDescription\\n\\ndata points following the main power stream data points in low wind speed period with high power generation data points with negative value wind speed data points with negative value power generation data points with low power generation at high wind speed period\\n\\nThe existence of type 2 data might be due to some physical problems, in communication channels. Type 3 data does not exist in reality and may be caused by anemometer that needs to be calibrated. Type 4 data is due to the fact that sometimes the wind turbine\\n\\ndisabled\\n\\nsensors\\n\\nor\\n\\ndata\\n\\ndistortion\\n\\n2\\n\\nthe electrical cannot generate enough power consumption of itself and was drawing (consuming) power from the grid. The existence of type 5 data might be due to the fact that not all turbines are always online during high wind speed period (especially near cut-off wind speed) and some wind turbines maybe disabled during that period. Another reason can be derived from [4], because a strong wind from wrong direction can make a turbine work at low efficiency. In sum, all types of data except type 1 data should be filtered out.\\n\\nto offset\\n\\nthe\\n\\nturbine\\n\\nFig.2. Scatter plot of total output of WPP and wind speed from MET1 (2010 Jan-Mar)\\n\\nB. Data Selection Process', metadata={'source': '/tmp/tmp06m3zwow/tmp.pdf'}), Document(page_content='to offset\\n\\nthe\\n\\nturbine\\n\\nFig.2. Scatter plot of total output of WPP and wind speed from MET1 (2010 Jan-Mar)\\n\\nB. Data Selection Process\\n\\nProbabilistic neural network (PNN) is a feed-forward neural network with supervised learning using Bayes decision rule and Parzen window [3]. PNN can be used for data classification. The structure of PNN is usually a two-layer model as shown in Fig. 3. In the pattern units, the distance between the input vector and the target vector will be calculated. A new vector will be generated to indicate how close the input is to the target vector. The summation units add these distances for each type of inputs to produce a vector of probabilities as the output of the network. The output unit generates a 1 for the target class and a 0 for the other classes with the use of a competing transfer function, which picks the maximum the vector of probabilities [4].\\n\\nFig.3. Structure of PNN\\n\\nIn this paper, PNN was applied to filter out invalid data in the raw data set. For example, data points in Fig.2 were classified into five types and the portion for each type of data is different based on statistical analysis. The order of proportion from the largest to the smallest is type 1, type 4, type 5, type 2, type 3. In the process of building PNN model, about 20% of the data points in Fig. 2 (1700 data points) were selected as training data set. The PNN model was trained using the sampled data.\\n\\nSince only type 1 data is the useful information and should be kept, there are two strategies in training PNN model. Method 1 is simpler, for which the classification results of PNN are assumed to have only two types. PNN is trained based on two groups: the first group is type 1; the second group includes type 2-5. 1700 data points are selected, among which 1540 were randomly selected from type 1 data points, the rest 160 data points were from type 2-5. In the training data set, the input data vector includes data of wind speed and wind power generation, and the target vector has only two elements, which are 1 (group 1) and 2 (group 2). And then, the rest data points (about 80%) were used as testing data set as input to be classified by the PNN model already built. The number of neurons in the input layer is equal to that of the output layer, which is 2. The training results are shown in Fig. 4 and Fig. 5.\\n\\nFig.4. Classification results of data in Fig.2 by method 1\\n\\nFig.5. Filtered scatter plot of Group 1 data points classified by method 1 (2010 Jan-Mar)\\n\\n3\\n\\nAs shown in Fig. 5, the classification result using method 1 is not ideal; PNN model could not succeed in diagnosing all the unwanted data. And the classification accuracy is 92.7%, which means the number of the correctly classified data points versus the number of type 1 data as shown in Fig. 2.\\n\\nIn method 2, there are five classification results of PNN, which are type 1, type 2, type 3, type 4, type 5. PNN is trained based on five types of data points as shown in Table I. In the 1700 selected data points, 1540 data points were sampled from type 1 data. For the rest 160 data points, according to the portion of each data type, number of data points sampled from type 2, 3, 4, 5 were 20, 10, 70 and 60 respectively. The number of neurons in the input layer is 2 and the number of neurons in the output layer is 5. And results done by testing the rest of the data set can be seen in Fig. 6 and Fig.7.\\n\\nFig.6. Classification results of data in Fig.2 by method 2\\n\\nFig.7. Filtered scatter plot of type 1 data points classified by method 2 (2010 Jan-Mar)\\n\\nIn Fig. 6, type 1 data can be separated from the testing data set as shown in the classification result and were plotted in Fig. 7. And PNN model built by method 2 could succeed in screening the raw data even though the power curve is not totally smooth. The classification accuracy is 96.5%, which is higher than that of method 1. The classification result using', metadata={'source': '/tmp/tmp06m3zwow/tmp.pdf'}), Document(page_content=\"method 2 has a better accuracy than the one performed using method 1, because simply combining type 2-5 data pints into a group will disturb the process of building PNN and it creates confusions in dividing the line between type 1 data and type 2- 5 data.\\n\\nWrong classification data points will decrease the accuracy of the prediction model. In the data preparation process of the WPP power prediction model, we adopted method 2 to train PNN model and the problem of wrong classification can be solved by improving the PNN's training data set. For example, after the PNN is built, data points with wrong classification results from testing data set can be added into the training data set. And then PNN model should be trained again with the expanded training data set in order to have more accurate classification ability.\\n\\nC. Data for Building Models\\n\\nIn this paper, the power prediction result of the WPP is based on wind speed and wind direction. At first, wind speed factor is a key point in determining the available power generated from a single wind turbine with a certain cross- sectional area [5]. The wind speed experienced by individual wind turbines is acquired by the anemometer and comes from the direction of horizontal axes of turbine’s hub. The hub is behind the blades, which has an effect of decreasing the natural wind speed. The wind speed acquired from the MET towers represents the natural wind speed at the location on the tower. Even though the height of the hub and MET tower are the same, they have different physical meanings. When we predict wind power generation, wind speed from turbine should be adopted as input information of the model. Secondly, wind direction (direction from which the wind blows) is another kind of useful information to predict wind power based on previous research results [6]. Wind can come from every direction when the wind speed is low. The higher the wind speed, the more uniform and more focused the wind direction. So during the same wind speed period, wind turbines can have different efficiencies due to different wind directions. But it is not convenient to predict the total power generation of the whole WPP by processing data information from all the turbines. It is better to find wind turbines from which the wind speed and wind direction can be most representative of the WPP area's wind situation. The wind speed situation (after data selection process) of the whole year of 2010 is shown in Table II. The data from 2010 Apr-Jun covers a wide range and has the largest mean value of wind speed, which is suitable for training neural network model and was researched in this paper.\\n\\nTABLE II 2010 WIND SPEED DATA ANALYSIS\\n\\nWind Speed (m/s) Avg.\\n\\nStd. Maximum\\n\\n2010 Jan-Mar\\n\\n6.945\\n\\n3.919\\n\\n21.362\\n\\n2010 Apr-Jun\\n\\n8.812\\n\\n3.927\\n\\n22.635\\n\\n2010 Jul-Sep\\n\\n6.371\\n\\n3.212\\n\\n19.552\\n\\n2010 Oct-Dec\\n\\n7.330\\n\\n4.117\\n\\n21.912\\n\\nWind directions of the two groups of wind turbines at 3/18/2010 10:00 pm and 4/10/2010 8:40 am are shown in Fig.\\n\\n4\\n\\n8 and Fig. 9 respectively. The arrows indicate the direction of the wind. The wind directions of Group 2 turbines are focused on a certain direction. The reason of the messy direction of Group 1 turbine is likely to be the data distortion due to the data transmission channel or bad performance sensors. The total output of WPP can be predicted according to only one or two turbine’s information [1]. Based on the filtered data set, the average wind speed of all the wind turbines can be acquired. By correlation method, the wind turbine which has the highest correlation value with the average wind speed can be found (turbine A as indicated in Fig.1) and thus turbine A is the one which has the most representative wind speed. Following the same method, the turbine which has the most representative wind direction can also be found (turbine B as indicate in Fig.1). In this paper, data acquired from turbine A and B will be used to predict the total output of the WPP.\", metadata={'source': '/tmp/tmp06m3zwow/tmp.pdf'}), Document(page_content=\"Fig.8. Wind direction at 3/18/2010 10:00 pm\\n\\nFig.9. Wind direction at 4/10/2010 8:40 am\\n\\nIII. MODEL DESIGN\\n\\nA. Complex-valued Recurrent Neural Network Model Structure\\n\\nData of wind speed and direction from turbine A and turbine B can be combined and expressed as a vector on a two-dimensional complex coordinates as shown in Fig. 10.\\n\\nThe wind vector can be expressed as equation (1). Paper [7] demonstrates that the prediction effect by using complex- valued neural network outperforms that of using real-valued neural network.\\n\\nv(cid:4652)(cid:1318) (cid:3404) vcosθ (cid:3397) i vsinθ\\n\\nFig.10. Wind vector\\n\\nInputs of recurrent neural network can be either a series of historical measured data or simulated data generated by the model as shown in Fig. 11. The advantage of this kind of model is that the output signal does not just rely on the current input signals of the system but it also has an internal memory in its training process. The disadvantage is that the training time of the recurrent neural network is longer than that of the static neural network. In this paper, we built a complex-valued recurrent neural network (CRNN) to predict the WPP's power generation. The CRNN can be trained under two kinds of modes— Parallel (P) mode and Series-Parallel (SP) mode, as seen in Fig.11. In the former mode, the simulated outputs p(cid:3556)(cid:4666)n (cid:3398) 2(cid:4667), p(cid:3556)(cid:4666)n (cid:3398) 1(cid:4667), p(cid:3556)(cid:4666)n(cid:4667) are fed back as input signals. In the SP mode, actual outputs in the previous time step p(cid:4666)n (cid:3398) 2(cid:4667), p(cid:4666)n (cid:3398) 1(cid:4667), p(cid:4666)n(cid:4667) are used. Paper [8] demonstrates that prediction model with parallel mode inputs will result in accumulation of error if the previous prediction results are not accurate.\\n\\nFig.11. Recurrent neural network training structure\\n\\nB. Basic Algorithm\\n\\nIn this paper, (cid:1868) indicates the power readings from MET tower, which represents the value of the effective power amount of the whole WPP transmitted to the grid. The (cid:1873) includes the wind speed vectors from representative wind\\n\\n(1)\\n\\n5\\n\\nturbines (turbine A and B). And, (cid:1866) indicates the time step of 10 minutes period. Usually a two-layer NN model can reasonably approximate any nonlinear function [9]. In this paper, a single hidden layer NN with fifteen neurons and one output was used. A bias of 1 was set initially. The longer the length of delay, the heavier the load of the training process has, which will also inevitably increase the training time of the model. In this paper, we trained the complex-valued recurrent neural network in 10-min, 20-min, 30-min, 40-min, 50-min, 60-min time delay modes. For the transfer function, log- sigmoid function was selected to be the hidden layer’s transfer function due to its efficiency; linear transfer function was used in the output layer as a convention. Levenberg-Marquardt back propagation algorithm is used as the training function for the whole recurrent neural network model. This method is typically used in minimization problems because it appears to be the fastest method in terms of convergence. The weights of each connection between neurons are adjusted in the training process until the errors are within the pre-determined range. To compare the performance of the two modes of recurrent neural network, the accuracy of the model can be evaluated by mean absolute error (MAE), as shown in (2), root mean squared error (RMSE), as shown in (3) and mean absolute percentage error (MAPE), as shown in (4). In (2) (3) (4), xi and x(cid:3112)(cid:3557) are the ith component of the actual power and predicted wind power respectively. And, n is the length of the vector.\\n\\nMAE (cid:3404)\\n\\n1 n\\n\\n(cid:2924)\\n\\n(cid:3533) |x(cid:2919) (cid:3398) x(cid:3112)(cid:3541) |\\n\\n(cid:2919)(cid:2880)(cid:2869)\\n\\n(2)\\n\\nRMSE (cid:3404) (cid:3497)\\n\\n1 n\", metadata={'source': '/tmp/tmp06m3zwow/tmp.pdf'}), Document(page_content=\"MAE (cid:3404)\\n\\n1 n\\n\\n(cid:2924)\\n\\n(cid:3533) |x(cid:2919) (cid:3398) x(cid:3112)(cid:3541) |\\n\\n(cid:2919)(cid:2880)(cid:2869)\\n\\n(2)\\n\\nRMSE (cid:3404) (cid:3497)\\n\\n1 n\\n\\n(cid:2924) (cid:3533)(cid:4666)x(cid:2919) (cid:3398) x(cid:3112)(cid:3557) (cid:4667)(cid:2870) (cid:2919)(cid:2880)(cid:2869)\\n\\n(3)\\n\\nMAPE (cid:3404)\\n\\n1 n\\n\\n(cid:2924) (cid:3533) (cid:3628) (cid:2919)(cid:2880)(cid:2869)\\n\\nx(cid:2919) (cid:3398) x(cid:3112)(cid:3557) x(cid:2919)\\n\\n(cid:3628) (cid:3400) 100\\n\\n%\\n\\n(4)\\n\\nIV. COMPARISON AND ANALYSIS OF PREDICTION RESULTS\\n\\nBased on section II, data as shown in Table III was selected to finish the WPP's power prediction model. Data of each group consists of wind speed, wind direction and wind power generation.\\n\\nTABLE III DATA DESCRIPTION\\n\\nData Group A\\n\\nB\\n\\nStart time\\n\\n4/1/2010 0:00\\n\\n4/1/2011 0:10\\n\\nEnd time\\n\\n5/8/2010 23:50 5/8/2011 23:50\\n\\nNo. of data points 5474\\n\\n5362\\n\\nDescription\\n\\nTraining data set Testing data set\\n\\nIn the modeling process, the Group A's data is used for training the model; the Group B's data is used for testing and validation of the model. In the training process of neural network, according to the principle of the neural network, training set data will be divided into two parts randomly, one part is for learning the relationship between input data and output data and building the model, which occupies 60% of the total data, the rest 40% data is reserved for validation of the model and for further adjusting value of its weights. So\\n\\nmodels built by a same training data set could be different due to neural network's randomness in training. In order to get more accurate results, each model was built by Group A data repeatedly for three times and the prediction results were tested by Group B data repeatedly for three times and then average values are computed. Results from the proposed model were compared with the actual values of the historical data. The error statistics of the prediction results by different time series SP mode CRNN is shown in Table IV and Table V. Model 1 denotes SP mode CRNN with only wind speed as input, Model 2 denotes SP mode CRNN with wind vectors as inputs. Table VI shows the error analysis of prediction results by complex-valued neural network (CVNN) and real-valued neural network model (RVNN).\\n\\n( MW)\\n\\nTABLE IV ERROR ANALYSIS I MAE\\n\\nRMSE\\n\\nInput type 10min 20min 30min 40min 50min 60min\\n\\nModel 1 9.901 10.156 11.205 13.371 13.931 14.691\\n\\nModel 2 7.874 8.635 9.258 9.422 9.423 9.58\\n\\nModel 1 13.613 14.067 16.268 16.331 18.256 18.584\\n\\nModel2 9.408 9.677 10.502 10.845 11.111 11.374\\n\\nTABLE V ERROR ANALYSIS II\\n\\nMAPE (%)\\n\\nStd. of Error (MW)\\n\\nInput type 10min 20min 30min 40min 50min 60min\\n\\nModel 1 12.163 14.549 18.149 22.777 24.448 29.192\\n\\nModel 2 11.204 12.091 12.753 13.872 17.772 18.091\\n\\nModel 1 10.722 11.801 13.834 15.362 17.255 19.522\\n\\nModel2 9.408 11.221 11.474 12.385 12.722 13.044\\n\\nTABLE VI ERROR ANALYSIS OF CVNN RMSE\\n\\nMAE\\n\\nStd. of Error\\n\\nMAPE\\n\\nCVNN RVNN\\n\\n14.149 16.210\\n\\n14.994 17.086\\n\\n14.635 30.672\\n\\n14.867% 22.535%\", metadata={'source': '/tmp/tmp06m3zwow/tmp.pdf'}), Document(page_content=\"Model2 9.408 11.221 11.474 12.385 12.722 13.044\\n\\nTABLE VI ERROR ANALYSIS OF CVNN RMSE\\n\\nMAE\\n\\nStd. of Error\\n\\nMAPE\\n\\nCVNN RVNN\\n\\n14.149 16.210\\n\\n14.994 17.086\\n\\n14.635 30.672\\n\\n14.867% 22.535%\\n\\nFrom Table IV, V and VI above, the results show that the 10-min delay mode of Model 2 has the best performance in the CRNN models and can be adopted to build power prediction models for WPP. The accuracy suggested by MAPE is 11.204%, which also outperforms the prediction results of CVNN and RVNN as shown in Table VI. In the CRNN models, the accuracy of CRNN's prediction results decreases with the increasing of the delay length in the model training process. The reason is that the wind is changing rapidly, so it is better to predict the wind power by referring the wind status in the nearest previous time. Apparently, the accuracy of prediction results and its consistency for different delay length are improved when the direction of wind is combined into input signals of the neural network. The prediction results of CVNN and RVNN models, which do not include time delay in their training data set, have worse prediction results even compared to Model 2 with 40-min delay. Fig. 12 shows the prediction results from 4/1/2011 1:20 am to 4/2/201110:40 am, where the predicted power generation points are very close to theose actual ones. Additionally, there are always some\\n\\n6\\n\\nprediction data points with large relative errors, which are larger than 100%. The characteristic of those data points are always generated during low wind speed period (below 4m/s) which is not important for wind power integration and can be ignored.\\n\\nThe whole prediction is shown in Fig. 13. Overall, most of the prediction values are smaller than the actual values. According to the errors of the prediction results, the power company can compensate the errors by allocating proper power reserve and make some adjustment in scheduling the wind power generation.\\n\\nFig.12 Prediction results by 10-min time delay SP mode RNN\\n\\nFig.13 Predicted wind power vs. actual wind power of 2011\\n\\nV. CONCLUSION\\n\\nThis paper describes a procedure of predicting total output of wind power plant (WPP) by neural networks. Probabilistic neural network (PNN) was applied to classify and screen the raw wind data for the training of neural network prediction models. And then certain representative wind turbines were selected as an input data source for modeling and to simplify the input signals to the model. In the last step, based on the previous wind power prediction experience [2-6], complex- valued recurrent neural network (CRNN) model was chosen to predict the total output of WPP with high accuracy.\\n\\nVI. ACKNOWLEDGMENT\\n\\nThis work was partially supported by the U.S. Department of Energy under Contract No. DE-AC36-08-GO28308 with the National Renewable Energy Laboratory and was also partially supported by U.S. NSF Grant 0844707.\\n\\nVII. REFERENCES [1] Yuan-Kang Wu, Jing-Shan Hong, “A literature review of wind forecasting technology in the world,” presented at 2007 IEEE Lausanne Power Tech Conf., pp.504-509.\\n\\n[2] Yih-Huei Wan, Erik Ela, and Kirsten Orwig, “Development of an Equivalent Wind Plant Power Curve,” NREL, Golden, CO, Tech. Rep. NREL/CP-550-48146, Jun.2010.\\n\\n[3] Kang Meei-Song, Chen Chao-Shun, Ke Yu-Lung and Lin Chia-Hung, “ Load Profile Synthesis and Wind Power Generation Prediction for an Isolated Power System,” IEEE Trans. on Industry Applications, vpl.43, pp.1459-1484, Nov. 2007.\\n\\n[4] http://www.mathworks.com/help/toolbox/nnet/ug/bss38ji-1.html. [5] Miller, A., Muljadi, E. and Zinger, D.S. “A Variable Wind Turbine Power Control,” IEEE Trans. Energy Conversion, vol. 12, no.2, 1997, pp. 181-186.\\n\\n[6] Shuhui Li, Donald C. Wunsch, Edgar A. O’Hair and Michael G. Giesselmann, “Using neural networks to estimate wind turbine power generation,” IEEE Trans. on Energy Conversion, vol. 16, pp.267-282, Sep.2001.\", metadata={'source': '/tmp/tmp06m3zwow/tmp.pdf'}), Document(page_content='[7] Takahiro Kitajima, Takashi Yasuno, “Output Prediction of Wind Power Generation System Using Complex-valued Neural Network,” in Proc. 2010 SICE Annual Conf., pp. 3610-3613.\\n\\n[8] Andrew Kusiak, Haiyang Zheng and Zhe Song, “Short-term prediction of wind farm power: a data mining approach,” IEEE Trans on Energy Conversion, vol.24, no.1, pp.125-135, Mar. 2009. Jagannathan Sarangapani, Neural Network Control of Nonlinear Discrete-Time Systems, Taylor and Francis Group, TJ213. S117, 2006.\\n\\n[9]\\n\\nVIII. BIOGRAPHIES\\n\\nZiqiao Liu received the M.S. degree from Beijing Institute of Technology, School of Automation, Beijing, China, in 2010. She is currently working toward the Department of Electrical and Computer Engineering, University of Denver, Colorado, USA. Her current research interests include wind power prediction, wind farm supervisory control.\\n\\nthe Ph.D. degree\\n\\nin\\n\\nWenzhong Gao (S’00–M’02–SM’03) received the M.S. and Ph.D. degrees in electrical and computer engineering specializing in electric power engineering from Georgia Institute of Technology, Atlanta, in 1999 and 2002, respectively. He is currently with the Department of Electrical and Computer Engineering, University of Denver, Colorado, USA. His current teaching and research interests include renewable energy and distributed generation, smart grid, power system protection, power electronics applications in power systems, power system modeling and simulation, and hybrid electric propulsion systems. He is an Editor for IEEE Transactions on Sustainable Energy. He is the General Chair for The IEEE Symposium on Power Electronics and Machines in Wind Applications (PEMWA 2012).\\n\\nYih-huie Wan received the B.S. degree from National Cheng Kung University, Taiwan, R.O.C., and the M.S. degree from Southern Illinois University, Carbondale, IL, in 1973 and 1979, respectively, both in electrical engineering. He is a Senior Engineer with the National Renewable Energy Laboratory (NREL), Golden, CO. His main expertise is in electric power system engineering, planning, and operation. Before joining NREL, he worked as an Electrical Engineer for an electric power company in Oklahoma for ten years in substation and distribution feeder design, transmission system\\n\\n7\\n\\nplanning, and bulk power transaction analysis. His work at NREL includes renewable energy technology assessment and integration of renewable energy technologies with the electric utility grid. He has also done work in renewable energy policy analysis, renewable energy resource assessment, and distributed generation. Mr. Wan is a Registered Professional Engineer in the State of Oklahoma.\\n\\nEduard Muljadi (M’82, SM’94, F’10) received his Ph.D. in electrical engineering from the University of Wisconsin at Madison. From 1988 to 1992, he taught at California State University at Fresno. In June 1992, he joined the National Renewable Energy Laboratory in Golden, Colorado. His research interests are the fields of electric machines, power electronics, and power systems, with an emphasis on renewable energy applications. He is a member of Eta Kappa Nu and Sigma X, and a fellow of the Institute of Electrical and Electronics Engineers (IEEE). He is involved in the activities of the IEEE Industry Application Society (IAS), Power Electronics Society, and Power and Energy Society (PES), and an editor of the IEEE Transactions on Energy Conversion.\\n\\nin\\n\\nDr. Muljadi is a member of various committees of the IAS, as well as a member of the Working Group on Renewable Technologies and the Task Force on Dynamic Performance of Wind Power Generation, both of the PES. He holds two patents in power conversion for renewable energy.', metadata={'source': '/tmp/tmp06m3zwow/tmp.pdf'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PDF_response = askQuestions(PDF_vector_store, chain, \"Summarize the content of this paper please\")\n",
        "print(PDF_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCUBPwVdbKge",
        "outputId": "30acf997-1cb2-4999-c967-366aabb12b3b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "MAE (cid:3404)\n",
            "\n",
            "1 n\n",
            "\n",
            "(cid:2924)\n",
            "\n",
            "(cid:3533) |x(cid:2919) (cid:3398) x(cid:3112)(cid:3541) |\n",
            "\n",
            "(cid:2919)(cid:2880)(cid:2869)\n",
            "\n",
            "(2)\n",
            "\n",
            "RMSE (cid:3404) (cid:3497)\n",
            "\n",
            "1 n\n",
            "\n",
            "(cid:2924) (cid:3533)(cid:4666)x(cid:2919) (cid:3398) x(cid:3112)(cid:3557) (cid:4667)(cid:2870) (cid:2919)(cid:2880)(cid:2869)\n",
            "\n",
            "(3)\n",
            "\n",
            "MAPE (cid:3404)\n",
            "\n",
            "1 n\n",
            "\n",
            "(cid:2924) (cid:3533) (cid:3628) (cid:2919)(cid:2880)(cid:2869)\n",
            "\n",
            "x(cid:2919) (cid:3398) x(cid:3112)(cid:3557) x(cid:2919)\n",
            "\n",
            "(cid:3628) (cid:3400) 100\n",
            "\n",
            "%\n",
            "\n",
            "(4)\n",
            "\n",
            "IV. COMPARISON AND ANALYSIS OF PREDICTION RESULTS\n",
            "\n",
            "Based on section II, data as shown in Table III was selected to finish the WPP's power prediction model. Data of each group consists of wind speed, wind direction and wind power generation.\n",
            "\n",
            "TABLE III DATA DESCRIPTION\n",
            "\n",
            "Data Group A\n",
            "\n",
            "B\n",
            "\n",
            "Start time\n",
            "\n",
            "4/1/2010 0:00\n",
            "\n",
            "4/1/2011 0:10\n",
            "\n",
            "End time\n",
            "\n",
            "5/8/2010 23:50 5/8/2011 23:50\n",
            "\n",
            "No. of data points 5474\n",
            "\n",
            "5362\n",
            "\n",
            "Description\n",
            "\n",
            "Training data set Testing data set\n",
            "\n",
            "In the modeling process, the Group A's data is used for training the model; the Group B's data is used for testing and validation of the model. In the training process of neural network, according to the principle of the neural network, training set data will be divided into two parts randomly, one part is for learning the relationship between input data and output data and building the model, which occupies 60% of the total data, the rest 40% data is reserved for validation of the model and for further adjusting value of its weights. So\n",
            "\n",
            "models built by a same training data set could be different due to neural network's randomness in training. In order to get more accurate results, each model was built by Group A data repeatedly for three times and the prediction results were tested by Group B data repeatedly for three times and then average values are computed. Results from the proposed model were compared with the actual values of the historical data. The error statistics of the prediction results by different time series SP mode CRNN is shown in Table IV and Table V. Model 1 denotes SP mode CRNN with only wind speed as input, Model 2 denotes SP mode CRNN with wind vectors as inputs. Table VI shows the error analysis of prediction results by complex-valued neural network (CVNN) and real-valued neural network model (RVNN).\n",
            "\n",
            "( MW)\n",
            "\n",
            "TABLE IV ERROR ANALYSIS I MAE\n",
            "\n",
            "RMSE\n",
            "\n",
            "Input type 10min 20min 30min 40min 50min 60min\n",
            "\n",
            "Model 1 9.901 10.156 11.205 13.371 13.931 14.691\n",
            "\n",
            "Model 2 7.874 8.635 9.258 9.422 9.423 9.58\n",
            "\n",
            "Model 1 13.613 14.067 16.268 16.331 18.256 18.584\n",
            "\n",
            "Model2 9.408 9.677 10.502 10.845 11.111 11.374\n",
            "\n",
            "TABLE V ERROR ANALYSIS II\n",
            "\n",
            "MAPE (%)\n",
            "\n",
            "Std. of Error (MW)\n",
            "\n",
            "Input type 10min 20min 30min 40min 50min 60min\n",
            "\n",
            "Model 1 12.163 14.549 18.149 22.777 24.448 29.192\n",
            "\n",
            "Model 2 11.204 12.091 12.753 13.872 17.772 18.091\n",
            "\n",
            "Model 1 10.722 11.801 13.834 15.362 17.255 19.522\n",
            "\n",
            "Model2 9.408 11.221 11.474 12.385 12.722 13.044\n",
            "\n",
            "TABLE VI ERROR ANALYSIS OF CVNN RMSE\n",
            "\n",
            "MAE\n",
            "\n",
            "Std. of Error\n",
            "\n",
            "MAPE\n",
            "\n",
            "CVNN RVNN\n",
            "\n",
            "14.149 16.210\n",
            "\n",
            "14.994 17.086\n",
            "\n",
            "14.635 30.672\n",
            "\n",
            "14.867% 22.535%\n",
            "\n",
            "[7] Takahiro Kitajima, Takashi Yasuno, “Output Prediction of Wind Power Generation System Using Complex-valued Neural Network,” in Proc. 2010 SICE Annual Conf., pp. 3610-3613.\n",
            "\n",
            "[8] Andrew Kusiak, Haiyang Zheng and Zhe Song, “Short-term prediction of wind farm power: a data mining approach,” IEEE Trans on Energy Conversion, vol.24, no.1, pp.125-135, Mar. 2009. Jagannathan Sarangapani, Neural Network Control of Nonlinear Discrete-Time Systems, Taylor and Francis Group, TJ213. S117, 2006.\n",
            "\n",
            "[9]\n",
            "\n",
            "VIII. BIOGRAPHIES\n",
            "\n",
            "Ziqiao Liu received the M.S. degree from Beijing Institute of Technology, School of Automation, Beijing, China, in 2010. She is currently working toward the Department of Electrical and Computer Engineering, University of Denver, Colorado, USA. Her current research interests include wind power prediction, wind farm supervisory control.\n",
            "\n",
            "the Ph.D. degree\n",
            "\n",
            "in\n",
            "\n",
            "Wenzhong Gao (S’00–M’02–SM’03) received the M.S. and Ph.D. degrees in electrical and computer engineering specializing in electric power engineering from Georgia Institute of Technology, Atlanta, in 1999 and 2002, respectively. He is currently with the Department of Electrical and Computer Engineering, University of Denver, Colorado, USA. His current teaching and research interests include renewable energy and distributed generation, smart grid, power system protection, power electronics applications in power systems, power system modeling and simulation, and hybrid electric propulsion systems. He is an Editor for IEEE Transactions on Sustainable Energy. He is the General Chair for The IEEE Symposium on Power Electronics and Machines in Wind Applications (PEMWA 2012).\n",
            "\n",
            "Yih-huie Wan received the B.S. degree from National Cheng Kung University, Taiwan, R.O.C., and the M.S. degree from Southern Illinois University, Carbondale, IL, in 1973 and 1979, respectively, both in electrical engineering. He is a Senior Engineer with the National Renewable Energy Laboratory (NREL), Golden, CO. His main expertise is in electric power system engineering, planning, and operation. Before joining NREL, he worked as an Electrical Engineer for an electric power company in Oklahoma for ten years in substation and distribution feeder design, transmission system\n",
            "\n",
            "7\n",
            "\n",
            "planning, and bulk power transaction analysis. His work at NREL includes renewable energy technology assessment and integration of renewable energy technologies with the electric utility grid. He has also done work in renewable energy policy analysis, renewable energy resource assessment, and distributed generation. Mr. Wan is a Registered Professional Engineer in the State of Oklahoma.\n",
            "\n",
            "Eduard Muljadi (M’82, SM’94, F’10) received his Ph.D. in electrical engineering from the University of Wisconsin at Madison. From 1988 to 1992, he taught at California State University at Fresno. In June 1992, he joined the National Renewable Energy Laboratory in Golden, Colorado. His research interests are the fields of electric machines, power electronics, and power systems, with an emphasis on renewable energy applications. He is a member of Eta Kappa Nu and Sigma X, and a fellow of the Institute of Electrical and Electronics Engineers (IEEE). He is involved in the activities of the IEEE Industry Application Society (IAS), Power Electronics Society, and Power and Energy Society (PES), and an editor of the IEEE Transactions on Energy Conversion.\n",
            "\n",
            "in\n",
            "\n",
            "Dr. Muljadi is a member of various committees of the IAS, as well as a member of the Working Group on Renewable Technologies and the Task Force on Dynamic Performance of Wind Power Generation, both of the PES. He holds two patents in power conversion for renewable energy.\n",
            "\n",
            "Fig.8. Wind direction at 3/18/2010 10:00 pm\n",
            "\n",
            "Fig.9. Wind direction at 4/10/2010 8:40 am\n",
            "\n",
            "III. MODEL DESIGN\n",
            "\n",
            "A. Complex-valued Recurrent Neural Network Model Structure\n",
            "\n",
            "Data of wind speed and direction from turbine A and turbine B can be combined and expressed as a vector on a two-dimensional complex coordinates as shown in Fig. 10.\n",
            "\n",
            "The wind vector can be expressed as equation (1). Paper [7] demonstrates that the prediction effect by using complex- valued neural network outperforms that of using real-valued neural network.\n",
            "\n",
            "v(cid:4652)(cid:1318) (cid:3404) vcosθ (cid:3397) i vsinθ\n",
            "\n",
            "Fig.10. Wind vector\n",
            "\n",
            "Inputs of recurrent neural network can be either a series of historical measured data or simulated data generated by the model as shown in Fig. 11. The advantage of this kind of model is that the output signal does not just rely on the current input signals of the system but it also has an internal memory in its training process. The disadvantage is that the training time of the recurrent neural network is longer than that of the static neural network. In this paper, we built a complex-valued recurrent neural network (CRNN) to predict the WPP's power generation. The CRNN can be trained under two kinds of modes— Parallel (P) mode and Series-Parallel (SP) mode, as seen in Fig.11. In the former mode, the simulated outputs p(cid:3556)(cid:4666)n (cid:3398) 2(cid:4667), p(cid:3556)(cid:4666)n (cid:3398) 1(cid:4667), p(cid:3556)(cid:4666)n(cid:4667) are fed back as input signals. In the SP mode, actual outputs in the previous time step p(cid:4666)n (cid:3398) 2(cid:4667), p(cid:4666)n (cid:3398) 1(cid:4667), p(cid:4666)n(cid:4667) are used. Paper [8] demonstrates that prediction model with parallel mode inputs will result in accumulation of error if the previous prediction results are not accurate.\n",
            "\n",
            "Fig.11. Recurrent neural network training structure\n",
            "\n",
            "B. Basic Algorithm\n",
            "\n",
            "In this paper, (cid:1868) indicates the power readings from MET tower, which represents the value of the effective power amount of the whole WPP transmitted to the grid. The (cid:1873) includes the wind speed vectors from representative wind\n",
            "\n",
            "(1)\n",
            "\n",
            "5\n",
            "\n",
            "turbines (turbine A and B). And, (cid:1866) indicates the time step of 10 minutes period. Usually a two-layer NN model can reasonably approximate any nonlinear function [9]. In this paper, a single hidden layer NN with fifteen neurons and one output was used. A bias of 1 was set initially. The longer the length of delay, the heavier the load of the training process has, which will also inevitably increase the training time of the model. In this paper, we trained the complex-valued recurrent neural network in 10-min, 20-min, 30-min, 40-min, 50-min, 60-min time delay modes. For the transfer function, log- sigmoid function was selected to be the hidden layer’s transfer function due to its efficiency; linear transfer function was used in the output layer as a convention. Levenberg-Marquardt back propagation algorithm is used as the training function for the whole recurrent neural network model. This method is typically used in minimization problems because it appears to be the fastest method in terms of convergence. The weights of each connection between neurons are adjusted in the training process until the errors are within the pre-determined range. To compare the performance of the two modes of recurrent neural network, the accuracy of the model can be evaluated by mean absolute error (MAE), as shown in (2), root mean squared error (RMSE), as shown in (3) and mean absolute percentage error (MAPE), as shown in (4). In (2) (3) (4), xi and x(cid:3112)(cid:3557) are the ith component of the actual power and predicted wind power respectively. And, n is the length of the vector.\n",
            "\n",
            "MAE (cid:3404)\n",
            "\n",
            "1 n\n",
            "\n",
            "(cid:2924)\n",
            "\n",
            "(cid:3533) |x(cid:2919) (cid:3398) x(cid:3112)(cid:3541) |\n",
            "\n",
            "(cid:2919)(cid:2880)(cid:2869)\n",
            "\n",
            "(2)\n",
            "\n",
            "RMSE (cid:3404) (cid:3497)\n",
            "\n",
            "1 n\n",
            "\n",
            "to offset\n",
            "\n",
            "the\n",
            "\n",
            "turbine\n",
            "\n",
            "Fig.2. Scatter plot of total output of WPP and wind speed from MET1 (2010 Jan-Mar)\n",
            "\n",
            "B. Data Selection Process\n",
            "\n",
            "Probabilistic neural network (PNN) is a feed-forward neural network with supervised learning using Bayes decision rule and Parzen window [3]. PNN can be used for data classification. The structure of PNN is usually a two-layer model as shown in Fig. 3. In the pattern units, the distance between the input vector and the target vector will be calculated. A new vector will be generated to indicate how close the input is to the target vector. The summation units add these distances for each type of inputs to produce a vector of probabilities as the output of the network. The output unit generates a 1 for the target class and a 0 for the other classes with the use of a competing transfer function, which picks the maximum the vector of probabilities [4].\n",
            "\n",
            "Fig.3. Structure of PNN\n",
            "\n",
            "In this paper, PNN was applied to filter out invalid data in the raw data set. For example, data points in Fig.2 were classified into five types and the portion for each type of data is different based on statistical analysis. The order of proportion from the largest to the smallest is type 1, type 4, type 5, type 2, type 3. In the process of building PNN model, about 20% of the data points in Fig. 2 (1700 data points) were selected as training data set. The PNN model was trained using the sampled data.\n",
            "\n",
            "Since only type 1 data is the useful information and should be kept, there are two strategies in training PNN model. Method 1 is simpler, for which the classification results of PNN are assumed to have only two types. PNN is trained based on two groups: the first group is type 1; the second group includes type 2-5. 1700 data points are selected, among which 1540 were randomly selected from type 1 data points, the rest 160 data points were from type 2-5. In the training data set, the input data vector includes data of wind speed and wind power generation, and the target vector has only two elements, which are 1 (group 1) and 2 (group 2). And then, the rest data points (about 80%) were used as testing data set as input to be classified by the PNN model already built. The number of neurons in the input layer is equal to that of the output layer, which is 2. The training results are shown in Fig. 4 and Fig. 5.\n",
            "\n",
            "Fig.4. Classification results of data in Fig.2 by method 1\n",
            "\n",
            "Fig.5. Filtered scatter plot of Group 1 data points classified by method 1 (2010 Jan-Mar)\n",
            "\n",
            "3\n",
            "\n",
            "As shown in Fig. 5, the classification result using method 1 is not ideal; PNN model could not succeed in diagnosing all the unwanted data. And the classification accuracy is 92.7%, which means the number of the correctly classified data points versus the number of type 1 data as shown in Fig. 2.\n",
            "\n",
            "In method 2, there are five classification results of PNN, which are type 1, type 2, type 3, type 4, type 5. PNN is trained based on five types of data points as shown in Table I. In the 1700 selected data points, 1540 data points were sampled from type 1 data. For the rest 160 data points, according to the portion of each data type, number of data points sampled from type 2, 3, 4, 5 were 20, 10, 70 and 60 respectively. The number of neurons in the input layer is 2 and the number of neurons in the output layer is 5. And results done by testing the rest of the data set can be seen in Fig. 6 and Fig.7.\n",
            "\n",
            "Fig.6. Classification results of data in Fig.2 by method 2\n",
            "\n",
            "Fig.7. Filtered scatter plot of type 1 data points classified by method 2 (2010 Jan-Mar)\n",
            "\n",
            "In Fig. 6, type 1 data can be separated from the testing data set as shown in the classification result and were plotted in Fig. 7. And PNN model built by method 2 could succeed in screening the raw data even though the power curve is not totally smooth. The classification accuracy is 96.5%, which is higher than that of method 1. The classification result using\n",
            "\n",
            "Question: Summarize the content of this paper please\n",
            "Helpful Answer: This paper presents a wind power prediction model using a complex-valued recurrent neural network (CRNN) with wind speed and direction as inputs. The model is trained under two modes, parallel (P) mode and series-parallel (SP) mode, and the accuracy of the model is evaluated using mean absolute error (MAE), root mean squared error (RMSE), and mean absolute percentage error (MAPE). The proposed model is compared with a real-valued neural network model\n"
          ]
        }
      ]
    }
  ]
}